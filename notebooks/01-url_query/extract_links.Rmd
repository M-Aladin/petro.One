---
title: "R Notebook"
output: html_notebook
---

https://stackoverflow.com/a/36723520/5270873

```{r}
library(xml2)
library(rvest)

URL <- "http://stackoverflow.com/questions/3746256/extract-links-from-webpage-using-r"

pg <- read_html(URL)

head(html_attr(html_nodes(pg, "a"), "href"))
```

Same as the chunk above:

```{r}
URL <- "http://stackoverflow.com/questions/3746256/extract-links-from-webpage-using-r"

read_html(URL) %>% 
    html_nodes("a") %>% 
    html_attr("href") %>% 
    head
```




https://stackoverflow.com/a/35247304/5270873

```{r}
library(rvest)

page = read_html("http://www.yelp.com/search?find_loc=New+York,+NY,+USA")
page %>% 
    html_nodes(".biz-name") %>% 
    html_attr('href')
```

## Using R to scrape the link address of a downloadable file from a web page?
https://stackoverflow.com/questions/31517121/using-r-to-scrape-the-link-address-of-a-downloadable-file-from-a-web-page

```{r}
library(rvest)
realtime.page <- "http://www.acleddata.com/data/realtime-data/"
realtime.html <- read_html(realtime.page)

realtime.html %>% 
    html_node(xpath = "/html/body/div/div/div/div[1]/div/article/div/ul[1]/li[2]/a") %>% 
    html_attr("href")
```


https://stackoverflow.com/a/31525104/5270873

```{r}
library(rvest)
library(stringr)

page <- read_html("http://www.acleddata.com/data/realtime-data/")

page %>%
  html_nodes("a") %>%        # find all links
  html_attr("href") %>%      # get the url
  # str_subset("\\.xlsx") %>%  # find those that end in xlsx
  str_subset("\\.pdf") %>%  # find those that end in xlsx
  .[[1]]                     # look at the first one
```

```{r}
library(rvest)

# Store web url
lego_movie <- read_html("http://www.imdb.com/title/tt1490017/")

#Scrape the website for the movie rating
rating <- lego_movie %>% 
  html_nodes("strong span") %>%
  html_text() %>%
  as.numeric()
rating
```

```{r}
# Scrape the website for the cast
cast <- lego_movie %>%
  html_nodes("#titleCast .itemprop span") %>%
  html_text()
cast
```


```{r}
#Scrape the website for the url of the movie poster
poster <- lego_movie %>%
  html_nodes("#img img") %>%
  html_attr("src")
poster
```


```{r}
library('rvest')

#Specifying the url for desired website to be scrapped
url <- 'http://www.imdb.com/search/title?count=100&release_date=2016,2016&title_type=feature'

#Reading the HTML code from the website
webpage <- read_html(url)

#Using CSS selectors to scrap the rankings section
rank_data_html <- html_nodes(webpage, '.text-primary')

#Converting the ranking data to text
rank_data <- html_text(rank_data_html)

#Let's have a look at the rankings
head(rank_data)

#Data-Preprocessing: Converting rankings to numerical
rank_data<-as.numeric(rank_data)

#Let's have another look at the rankings
head(rank_data)


#Using CSS selectors to scrap the title section
title_data_html <- html_nodes(webpage, '.lister-item-header a')

#Converting the title data to text
title_data <- html_text(title_data_html)

#Let's have a look at the title
head(title_data)

#Using CSS selectors to scrap the description section
description_data_html <- html_nodes(webpage, '.ratings-bar+ .text-muted')

#Converting the description data to text
description_data <- html_text(description_data_html)

#Let's have a look at the description data
# head(description_data)

#Data-Preprocessing: removing '\n'
description_data<-gsub("\n","",description_data)

#Let's have another look at the description data 
head(description_data)

#Using CSS selectors to scrap the Movie runtime section
runtime_data_html <- html_nodes(webpage, '.text-muted .runtime')

#Converting the runtime data to text
runtime_data <- html_text(runtime_data_html)

#Let's have a look at the runtime
head(runtime_data)

#Data-Preprocessing: removing mins and converting it to numerical

runtime_data<-gsub(" min","",runtime_data)
runtime_data<-as.numeric(runtime_data)
head(runtime_data)

#Using CSS selectors to scrap the Movie genre section
genre_data_html <- html_nodes(webpage, '.genre')

#Converting the genre data to text
genre_data <- html_text(genre_data_html)

#Let's have a look at the runtime
# head(genre_data)

#Data-Preprocessing: removing \n
genre_data<-gsub("\n","",genre_data)

#Data-Preprocessing: removing excess spaces
genre_data<-gsub(" ", "",genre_data)

#taking only the first genre of each movie
genre_data<-gsub(",.*", "", genre_data)

#Convering each genre from text to factor
genre_data<-as.factor(genre_data)

#Let's have another look at the genre data
head(genre_data)

#Using CSS selectors to scrap the IMDB rating section
rating_data_html <- html_nodes(webpage, '.ratings-imdb-rating strong')

#Converting the ratings data to text
rating_data <- html_text(rating_data_html)

#Let's have a look at the ratings
# head(rating_data)

#Data-Preprocessing: converting ratings to numerical
rating_data<-as.numeric(rating_data)

#Let's have another look at the ratings data
head(rating_data)


#Using CSS selectors to scrap the votes section
votes_data_html <- html_nodes(webpage, '.sort-num_votes-visible span:nth-child(2)')

#Converting the votes data to text
votes_data <- html_text(votes_data_html)

#Let's have a look at the votes data
head(votes_data)

#Data-Preprocessing: removing commas
votes_data<-gsub(",","",votes_data)

#Data-Preprocessing: converting votes to numerical
votes_data<-as.numeric(votes_data)

#Let's have another look at the votes data
head(votes_data)


#Using CSS selectors to scrap the directors section
directors_data_html <- html_nodes(webpage, '.text-muted+ p a:nth-child(1)')

#Converting the directors data to text
directors_data <- html_text(directors_data_html)

#Let's have a look at the directors data
head(directors_data)


#Using CSS selectors to scrap the actors section
actors_data_html <- html_nodes(webpage,'.lister-item-content .ghost+ a')

#Converting the gross actors data to text
actors_data <- html_text(actors_data_html)

#Let's have a look at the actors data
head(actors_data)

#Data-Preprocessing: converting actors data into factors
actors_data <- as.factor(actors_data)
head(actors_data)
```



```{r}
library(rvest)

x <- "https://en.wikipedia.org/wiki/Academy_Award_for_Best_Picture" %>% 
  read_html() %>% 
  html_nodes("#mw-content-text table:nth-child(55)")
    # html_nodes("#mw-content-text table")

html_table(x)

```


```{r}
trace(rvest:::html_table.xml_node, quote({ 
  values      <- lapply(lapply(cells, html_node, "a"), html_attr, name = "href")
  values[[1]] <- html_text(cells[[1]])
}), at = 14)
```
html_table essentially extracts the cells of the html table and runs html_text on them. All we need to do is replace that by extracting the <a> tag from each cell and running html_attr(., "href") instead.

```{r}
html_table(x)
```


```{r}
library(rvest)

url <- "http://www.ajnr.org/content/30/7/1402.full"
page <- read_html(url)

# First find all the urls
table_urls <- page %>% 
  html_nodes(".table-inline li:nth-child(1) a") %>%
  html_attr("href") %>%
  xml2::url_absolute(url)

table_urls
# Then loop over the urls, downloading & extracting the table
# lapply(table_urls, . %>% 
#            read_html() %>% 
#            html_table())
```


```{r}
library(rvest)
library(XML)

main_url <- "http://www.ajnr.org/content/30/7/1402/"
urls <- paste(main_url, c("T1.expansion", "T2.expansion", "T3.expansion", "T4.expansion"), ".html", sep = "")

urls 
tables <- list()
for(i in seq_along(urls))
{
  total <- readHTMLTable(urls[i])
  n.rows <- unlist(lapply(total, function(t) dim(t)[1]))
  tables[[i]] <- as.data.frame(total[[which.max(n.rows)]])
}
tables
```

```{r}
readHTMLTable(urls[1])
```


```{r}
library(rvest)
library(XML)
library(httr)

url <- "http://en.wikipedia.org/wiki/List_of_The_Simpsons_episodes"
doc <- content(GET(url))

getHrefs <- function(node, encoding) {  
  x <- xmlChildren(node)$a 
  if (!is.null(x)) paste0("http://", parseURI(url)$server, xmlGetAttr(x, "href"), " | ", xmlValue(x) ) else xmlValue(xmlChildren(node)$text) 
}

tab <- readHTMLTable(doc, which = 3, elFun = getHrefs)
head(tab[, 1:4])
```



## with error


```{r}
library(rvest)
library(XML)

pg <- htmlParse("http://www.bvl.com.pe/includes/empresas_todas.dat")
pg %>% 
    html_nodes("a") %>% 
    html_attr("href")
```


```{r}

links <- function(URL) 
{
    getLinks <- function() {
        links <- character()
        list(a = function(node, ...) {
                links <<- c(links, xmlGetAttr(node, "href"))
                node
             },
             links = function() links)
        }
    h1 <- getLinks()
    htmlTreeParse(URL, handlers = h1)
    h1$links()
}

links("https://www.bvl.com.pe/listado-de-empresas")
```



```{r}
# Option 1
library(RCurl)
getHTMLLinks('https://www.bvl.com.pe/listado-de-empresas/')


```

```{r}
# Option 2
library(rvest)
library(pipeR) # %>>% will be faster than %>%
html("http://www.bvl.com.pe/includes/empresas_todas.dat")%>>% html_nodes("a") %>>% html_attr("href")
```
